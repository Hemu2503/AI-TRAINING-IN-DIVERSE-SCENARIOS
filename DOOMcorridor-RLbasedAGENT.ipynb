{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "750fcee2-8073-4ab5-91e2-51975be22320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from vizdoom) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from vizdoom) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from vizdoom) (2.6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\heman\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\Lib\\site-packages\\torch-2.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\heman\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\Lib\\site-packages\\torch-2.4.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "492c2ee9-0eaf-4374-8cd2-c191bd4993b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ViZDoom' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!cd github & git clone https://github.com/mwydmuch/ViZDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f52ae29b-6a2a-4e75-b260-4736691d846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vizdoom for game env\n",
    "from vizdoom import * \n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time \n",
    "# Import numpy for identity matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22894e4e-61c4-4cbe-8890-cd36f34edbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup game\n",
    "game = DoomGame()\n",
    "game.load_config(r'C:\\DOOMgame-using-RLagent-main\\github\\ViZDoom\\scenarios\\deadly_corridor.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3687adf0-dea8-4966-8c74-bf312a636999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set of actions we can take in the environment\n",
    "actions = np.identity(7, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dcfd165d-47a1-4c2c-8e03-2d0d4e11ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f433d6e6-a66a-4112-8f70-9284e7b33c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.game_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b37eb032-de5a-4c42-89ca-83f9dbe3994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -3.5984649658203125\n",
      "reward: -4.7531890869140625\n",
      "reward: 3.674285888671875\n",
      "reward: 5.9538726806640625\n",
      "reward: 3.9779205322265625\n",
      "reward: 2.6656341552734375\n",
      "reward: 2.6337890625\n",
      "reward: -7.1376800537109375\n",
      "reward: -17.502883911132812\n",
      "reward: -1.889068603515625\n",
      "reward: -0.0213470458984375\n",
      "reward: 7.0593719482421875\n",
      "reward: 8.468719482421875\n",
      "reward: 5.712188720703125\n",
      "reward: 3.208404541015625\n",
      "reward: 7.7231597900390625\n",
      "reward: -91.70016479492188\n",
      "Result: -75.52545166015625\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.8306884765625\n",
      "reward: -16.701828002929688\n",
      "reward: -0.1110992431640625\n",
      "reward: 6.780731201171875\n",
      "reward: 8.227874755859375\n",
      "reward: 5.549713134765625\n",
      "reward: 10.116897583007812\n",
      "reward: 3.797149658203125\n",
      "reward: -7.1597747802734375\n",
      "reward: -5.5988616943359375\n",
      "reward: 4.9531097412109375\n",
      "reward: 13.578109741210938\n",
      "reward: 12.683456420898438\n",
      "reward: 2.181304931640625\n",
      "reward: -1.8758697509765625\n",
      "reward: -0.2917022705078125\n",
      "reward: -2.32421875\n",
      "reward: -101.20883178710938\n",
      "Result: -66.57315063476562\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.8358306884765625\n",
      "reward: -9.12493896484375\n",
      "reward: -4.29473876953125\n",
      "reward: -3.3857879638671875\n",
      "reward: -0.0172119140625\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 7.110260009765625\n",
      "reward: 8.734024047851562\n",
      "reward: 5.9983673095703125\n",
      "reward: 4.2501373291015625\n",
      "reward: 2.4477996826171875\n",
      "reward: 2.0322265625\n",
      "reward: 2.6481781005859375\n",
      "reward: 3.042205810546875\n",
      "reward: 2.477264404296875\n",
      "reward: -99.08256530761719\n",
      "Result: -76.32894897460938\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -2.0693511962890625\n",
      "reward: -6.5109405517578125\n",
      "reward: -4.3918304443359375\n",
      "reward: -3.00347900390625\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.0001678466796875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.01873779296875\n",
      "reward: 5.3359832763671875\n",
      "reward: 12.313812255859375\n",
      "reward: -96.921875\n",
      "Result: -95.26658630371094\n",
      "reward: 0.0\n",
      "reward: 0.78125\n",
      "reward: -3.0991973876953125\n",
      "reward: -8.713363647460938\n",
      "reward: -1.8557281494140625\n",
      "reward: 6.58111572265625\n",
      "reward: 8.174667358398438\n",
      "reward: 4.3330078125\n",
      "reward: 8.110107421875\n",
      "reward: 7.6914825439453125\n",
      "reward: 3.850677490234375\n",
      "reward: 2.97650146484375\n",
      "reward: -4.616424560546875\n",
      "reward: -13.881881713867188\n",
      "reward: -13.0712890625\n",
      "reward: -13.177490234375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.061737060546875\n",
      "reward: -0.0195770263671875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 7.110107421875\n",
      "reward: 8.529571533203125\n",
      "reward: 12.836761474609375\n",
      "reward: 13.000625610351562\n",
      "reward: 11.02850341796875\n",
      "reward: 6.9283294677734375\n",
      "reward: 4.463653564453125\n",
      "reward: -3.3444366455078125\n",
      "reward: -3.9931182861328125\n",
      "reward: -1.6502685546875\n",
      "reward: 0.4778900146484375\n",
      "reward: -2.8293914794921875\n",
      "reward: -97.32048034667969\n",
      "Result: -60.7601318359375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -9.287857055664062\n",
      "reward: -4.40673828125\n",
      "reward: 4.141265869140625\n",
      "reward: 6.5289764404296875\n",
      "reward: 9.955978393554688\n",
      "reward: 2.1781768798828125\n",
      "reward: -2.2665863037109375\n",
      "reward: -1.5289154052734375\n",
      "reward: -104.11979675292969\n",
      "Result: -98.80549621582031\n",
      "reward: 0.0\n",
      "reward: 0.78125\n",
      "reward: -1.7477874755859375\n",
      "reward: -13.358261108398438\n",
      "reward: -0.030059814453125\n",
      "reward: -0.9059906005859375\n",
      "reward: -0.675201416015625\n",
      "reward: 1.6420440673828125\n",
      "reward: 1.966217041015625\n",
      "reward: 8.226531982421875\n",
      "reward: 7.518096923828125\n",
      "reward: -12.872940063476562\n",
      "reward: -6.5406036376953125\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.6541748046875\n",
      "reward: 1.9843292236328125\n",
      "reward: -0.3160400390625\n",
      "reward: -3.3020782470703125\n",
      "reward: -100.0\n",
      "Result: -115.976318359375\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -2.0693511962890625\n",
      "reward: -6.5109405517578125\n",
      "reward: -7.395294189453125\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.0001983642578125\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.0024871826171875\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 4.428680419921875\n",
      "reward: 6.7166290283203125\n",
      "reward: 8.01483154296875\n",
      "reward: 7.3909454345703125\n",
      "reward: 3.5608978271484375\n",
      "reward: 4.8169708251953125\n",
      "reward: -94.99864196777344\n",
      "Result: -76.04795837402344\n",
      "reward: 0.0\n",
      "reward: -0.78125\n",
      "reward: -2.458099365234375\n",
      "reward: -8.771881103515625\n",
      "reward: -3.7818450927734375\n",
      "reward: 0.0\n",
      "reward: 2.256072998046875\n",
      "reward: 2.706390380859375\n",
      "reward: -0.430877685546875\n",
      "reward: 5.2368011474609375\n",
      "reward: 7.057159423828125\n",
      "reward: 2.1433258056640625\n",
      "reward: -1.9534454345703125\n",
      "reward: 3.5926361083984375\n",
      "reward: 11.288497924804688\n",
      "reward: 4.059417724609375\n",
      "reward: 2.1805572509765625\n",
      "reward: 2.9615631103515625\n",
      "reward: 1.99749755859375\n",
      "reward: 1.2449951171875\n",
      "reward: -99.85914611816406\n",
      "Result: -71.31163024902344\n",
      "reward: 0.0\n",
      "reward: -0.78125\n",
      "reward: -2.458099365234375\n",
      "reward: -1.6581268310546875\n",
      "reward: -7.416168212890625\n",
      "reward: -2.85931396484375\n",
      "reward: 0.718170166015625\n",
      "reward: 7.3079376220703125\n",
      "reward: 8.620574951171875\n",
      "reward: 3.9528045654296875\n",
      "reward: -5.6253662109375\n",
      "reward: -10.6373291015625\n",
      "reward: -5.076019287109375\n",
      "reward: 0.0\n",
      "reward: 6.992431640625\n",
      "reward: 7.0439605712890625\n",
      "reward: -0.4379730224609375\n",
      "reward: 0.71087646484375\n",
      "reward: 1.5624237060546875\n",
      "reward: 1.0538177490234375\n",
      "reward: 0.71075439453125\n",
      "reward: 0.479278564453125\n",
      "reward: -3.778656005859375\n",
      "reward: -0.601226806640625\n",
      "reward: 1.7483062744140625\n",
      "reward: -4.684234619140625\n",
      "reward: -4.94647216796875\n",
      "reward: -3.3365631103515625\n",
      "reward: -2.5833892822265625\n",
      "reward: 0.0\n",
      "reward: 5.684295654296875\n",
      "reward: 2.720458984375\n",
      "reward: 5.0599517822265625\n",
      "reward: 10.176528930664062\n",
      "reward: 9.018051147460938\n",
      "reward: 12.247802734375\n",
      "reward: 8.096237182617188\n",
      "reward: 3.0909881591796875\n",
      "reward: 0.7288818359375\n",
      "reward: -94.33734130859375\n",
      "Result: -53.49299621582031\n"
     ]
    }
   ],
   "source": [
    "# Loop through episodes \n",
    "episodes = 10 \n",
    "for episode in range(episodes): \n",
    "    # Create a new episode or game \n",
    "    game.new_episode()\n",
    "    # Check the game isn't done \n",
    "    while not game.is_episode_finished(): \n",
    "        # Get the game state \n",
    "        state = game.get_state()\n",
    "        # Get the game image \n",
    "        img = state.screen_buffer\n",
    "        # Get the game variables - ammo\n",
    "        info = state.game_variables\n",
    "        # Take an action\n",
    "        reward = game.make_action(random.choice(actions),4)\n",
    "        # Print rewward \n",
    "        print('reward:', reward) \n",
    "        time.sleep(0.02)\n",
    "    print('Result:', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "41b91823-7f6f-472e-973e-bebcf0c95bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aecbff6a-c71e-4012-8342-bc7d84b99675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\heman\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\Lib\\site-packages\\torch-2.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\heman\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\Lib\\site-packages\\torch-2.4.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c352d76a-07be-4abb-b1a4-7d851bf08239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gym import Env\n",
    "# Import gym spaces \n",
    "from gym.spaces import Discrete, Box\n",
    "# Import opencv \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "857da86f-8068-4ff2-8ae2-504a99ea768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game.get_state().screen_buffer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f3b97e80-5c1e-4e20-a078-6413437fecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vizdoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    # Function that is called when we start the env\n",
    "    def __init__(self, render=False, config= r'C:\\DOOMgame-using-RLagent-main\\github\\ViZDoom\\scenarios\\deadly_corridor.cfg'): \n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        # Setup the game \n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "        \n",
    "        # Render frame logic\n",
    "        if render == False: \n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        # Start the game \n",
    "        self.game.init()\n",
    "        \n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \n",
    "        self.action_space = Discrete(7)\n",
    "        \n",
    "        # Game variables: HEALTH DAMAGE_TAKEN HITCOUNT SELECTED_WEAPON_AMMO\n",
    "        self.damage_taken = 0\n",
    "        self.hitcount = 0\n",
    "        self.ammo = 52 ## CHANGED\n",
    "        \n",
    "        \n",
    "    # This is how we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step \n",
    "        actions = np.identity(7)\n",
    "        movement_reward = self.game.make_action(actions[action], 4) \n",
    "        \n",
    "        reward = 0 \n",
    "        # Get all the other stuff we need to retun \n",
    "        if self.game.get_state(): \n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            \n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables\n",
    "            health, damage_taken, hitcount, ammo = game_variables\n",
    "            \n",
    "            # Calculate reward deltas\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken\n",
    "            self.damage_taken = damage_taken\n",
    "            hitcount_delta = hitcount - self.hitcount\n",
    "            self.hitcount = hitcount\n",
    "            ammo_delta = ammo - self.ammo\n",
    "            self.ammo = ammo\n",
    "            \n",
    "            reward = movement_reward + damage_taken_delta*10 + hitcount_delta*200  + ammo_delta*5 \n",
    "            info = ammo\n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0 \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info \n",
    "    \n",
    "    # Define how to render the game or environment \n",
    "    def render(): \n",
    "        pass\n",
    "    \n",
    "    # What happens when we start a new game \n",
    "    def reset(self): \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    # Grayscale the game frame and resize it \n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    \n",
    "    # Call to close down the game\n",
    "    def close(self): \n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8836339b-680c-41b2-ba66-9d0e51446411",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6ccefc5b-b75e-4d1b-b89f-dcf94adf652f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[32],\n",
       "         [33],\n",
       "         [25],\n",
       "         ...,\n",
       "         [27],\n",
       "         [23],\n",
       "         [24]],\n",
       " \n",
       "        [[27],\n",
       "         [33],\n",
       "         [23],\n",
       "         ...,\n",
       "         [24],\n",
       "         [24],\n",
       "         [24]],\n",
       " \n",
       "        [[20],\n",
       "         [35],\n",
       "         [23],\n",
       "         ...,\n",
       "         [24],\n",
       "         [24],\n",
       "         [24]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[75],\n",
       "         [63],\n",
       "         [62],\n",
       "         ...,\n",
       "         [44],\n",
       "         [71],\n",
       "         [60]],\n",
       " \n",
       "        [[15],\n",
       "         [48],\n",
       "         [47],\n",
       "         ...,\n",
       "         [49],\n",
       "         [69],\n",
       "         [47]],\n",
       " \n",
       "        [[22],\n",
       "         [14],\n",
       "         [26],\n",
       "         ...,\n",
       "         [57],\n",
       "         [37],\n",
       "         [39]]], dtype=uint8),\n",
       " 0.0,\n",
       " False,\n",
       " {'info': 52.0})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "882c7629-a1ea-435c-8e12-0cb5e78d30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "32dbabf5-d5b6-4cfd-9f61-85f03a3df18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[32],\n",
       "        [33],\n",
       "        [25],\n",
       "        ...,\n",
       "        [27],\n",
       "        [23],\n",
       "        [24]],\n",
       "\n",
       "       [[27],\n",
       "        [33],\n",
       "        [23],\n",
       "        ...,\n",
       "        [24],\n",
       "        [24],\n",
       "        [24]],\n",
       "\n",
       "       [[20],\n",
       "        [35],\n",
       "        [23],\n",
       "        ...,\n",
       "        [24],\n",
       "        [24],\n",
       "        [24]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[75],\n",
       "        [63],\n",
       "        [62],\n",
       "        ...,\n",
       "        [44],\n",
       "        [71],\n",
       "        [60]],\n",
       "\n",
       "       [[15],\n",
       "        [48],\n",
       "        [47],\n",
       "        ...,\n",
       "        [49],\n",
       "        [69],\n",
       "        [47]],\n",
       "\n",
       "       [[22],\n",
       "        [14],\n",
       "        [26],\n",
       "        ...,\n",
       "        [57],\n",
       "        [37],\n",
       "        [39]]], dtype=uint8)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9c013c96-ded7-44ec-a29c-aabf646c74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "09d6f05f-6ae4-41aa-a990-588c5fc4efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Environment checker\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ccaf3339-b6ee-4e66-9614-ca7e2416743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5d286161-9e3d-439f-9e2b-32afd8d980fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4a0674f6-c617-44e6-847a-05621193fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3ff5561a-165c-478a-a771-2926cc1eb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.imshow(cv2.cvtColor(state, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "df7b1fcd-bfcd-4264-bb1b-e5dbdf436811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (2.5.0.dev20240821+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (0.20.0.dev20240821+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (2.4.0.dev20240821+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from torch) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heman\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\heman\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\Lib\\site-packages\\torch-2.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\heman\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\Lib\\site-packages\\torch-2.4.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "78cc5083-59ea-4a09-ac7d-25ebda8087d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines3[extra]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "28b475b7-a949-4d34-a6cb-eaa3c137ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file nav\n",
    "import os \n",
    "# Import callback class from sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c2036c89-18d0-48c0-b184-f0f1ea992711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "455ef84d-9635-4f02-bfe0-cd59009aa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_corridor'\n",
    "LOG_DIR = './logs/log_corridor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8445abcd-4652-4e66-bf3f-3103e83567d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2defadce-c5c9-468f-b9c7-45777b1906d7",
   "metadata": {},
   "source": [
    "#training using circulum learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0833fdd5-47ad-465a-bfa0-baf2729a0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import ppo for training\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "359d0abf-0fbe-4cde-84e8-437de4ac1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment\n",
    "env = VizDoomGym(config = r'C:\\DOOMgame-using-RLagent-main\\github\\ViZDoom\\scenarios\\deadly_corridor.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d9fba12d-9e6f-4f57-bd5d-dea462135e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heman\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f78a4b-f1dc-487b-a346-09a616f8450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/log_corridor\\PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32       |\n",
      "|    ep_rew_mean     | 10.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 36.3      |\n",
      "|    ep_rew_mean          | 19.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 21        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 189       |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4965789 |\n",
      "|    clip_fraction        | 0.416     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.88     |\n",
      "|    explained_variance   | -1.12e-05 |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 2.11e+04  |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.0361    |\n",
      "|    value_loss           | 3.67e+04  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.8        |\n",
      "|    ep_rew_mean          | 11.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 348         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038698126 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.24e+04    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0466      |\n",
      "|    value_loss           | 2.82e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 20.3       |\n",
      "|    ep_rew_mean          | 73         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 472        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56571007 |\n",
      "|    clip_fraction        | 0.519      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.68      |\n",
      "|    explained_variance   | 0.219      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.13e+04   |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0646     |\n",
      "|    value_loss           | 2.98e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.6        |\n",
      "|    ep_rew_mean          | 121         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019387428 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.9e+04     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0187      |\n",
      "|    value_loss           | 4.71e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20          |\n",
      "|    ep_rew_mean          | 104         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 668         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057809964 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 8.82e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00427     |\n",
      "|    value_loss           | 3.07e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.4        |\n",
      "|    ep_rew_mean          | 140         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 759         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018381212 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.09e+04    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    value_loss           | 3.25e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.2        |\n",
      "|    ep_rew_mean          | 206         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 851         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025447799 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.53e+04    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.000572    |\n",
      "|    value_loss           | 3.06e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 18.3        |\n",
      "|    ep_rew_mean          | 238         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 983         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030826662 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.831      |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 6.65e+03    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00462     |\n",
      "|    value_loss           | 2.59e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 18.5        |\n",
      "|    ep_rew_mean          | 263         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1077        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010473261 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.73       |\n",
      "|    explained_variance   | 0.55        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.55e+04    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.0027      |\n",
      "|    value_loss           | 2.67e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17.1        |\n",
      "|    ep_rew_mean          | 234         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 1171        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013237539 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 9.23e+03    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00156     |\n",
      "|    value_loss           | 2.34e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 16.7        |\n",
      "|    ep_rew_mean          | 230         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1246        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013670791 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.08e+04    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00415     |\n",
      "|    value_loss           | 2.85e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 16          |\n",
      "|    ep_rew_mean          | 241         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1993        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023934178 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.527      |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.75e+04    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.00354     |\n",
      "|    value_loss           | 2.45e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17.7        |\n",
      "|    ep_rew_mean          | 272         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 2056        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026354408 |\n",
      "|    clip_fraction        | 0.0715      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.359      |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.05e+04    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00156     |\n",
      "|    value_loss           | 2.53e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 18.2       |\n",
      "|    ep_rew_mean          | 279        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 2146       |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01210602 |\n",
      "|    clip_fraction        | 0.0763     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.322     |\n",
      "|    explained_variance   | 0.564      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.76e+04   |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | 0.00473    |\n",
      "|    value_loss           | 2.8e+04    |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "04479054-cbb6-4f71-a824-c72312d97869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5819e763-2387-4264-9a62-7a0f7b458760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reload model from disc\n",
    "model = PPO.load('./train/train_corridor/best_model_90000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0429e528-6697-4e1c-8d8c-48ab82123c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create rendered environment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b5d22dfa-7064-4c30-8222-269c0cafd789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "90bfc060-aeb7-4c0b-9e7a-3ff0d13dc80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4269256591796875"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68f3da-ad09-4a74-b74b-aee2b022bb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0b2caf71-bca9-4f90-b4d5-3a91b8ddb625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 0 is 146.13624572753906\n",
      "Total Reward for episode 1 is 217.1281280517578\n",
      "Total Reward for episode 2 is -255.98895263671875\n",
      "Total Reward for episode 3 is -33.233673095703125\n",
      "Total Reward for episode 4 is -65.98152160644531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(5): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.80)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "232e1f1e-eb8e-4b7d-a931-abff8d3e035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2969383b-2311-4a6c-8637-6c74b280eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "dea13123-3087-4d14-a464-41a343a52517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 6],\n",
       "         [10],\n",
       "         [ 9],\n",
       "         ...,\n",
       "         [ 6],\n",
       "         [11],\n",
       "         [ 4]],\n",
       " \n",
       "        [[ 9],\n",
       "         [ 9],\n",
       "         [ 6],\n",
       "         ...,\n",
       "         [ 6],\n",
       "         [ 6],\n",
       "         [ 4]],\n",
       " \n",
       "        [[10],\n",
       "         [ 7],\n",
       "         [ 7],\n",
       "         ...,\n",
       "         [ 6],\n",
       "         [ 6],\n",
       "         [ 9]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[75],\n",
       "         [63],\n",
       "         [62],\n",
       "         ...,\n",
       "         [44],\n",
       "         [71],\n",
       "         [60]],\n",
       " \n",
       "        [[15],\n",
       "         [48],\n",
       "         [47],\n",
       "         ...,\n",
       "         [49],\n",
       "         [69],\n",
       "         [47]],\n",
       " \n",
       "        [[22],\n",
       "         [14],\n",
       "         [26],\n",
       "         ...,\n",
       "         [57],\n",
       "         [37],\n",
       "         [39]]], dtype=uint8),\n",
       " 0.0,\n",
       " False,\n",
       " {'info': 26.0})"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fbcc59a2-0cf1-49b7-a9fa-4c3be456a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9afea5-f606-42a5-8a2f-bbafb74a5a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5d411-723f-4b79-843c-a2d946ae6cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
